{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sahilsingh0808/gpt-llm-trainer/blob/patches_sahil/gpt_llm_trainer_v2%2C_with_GPT_3_5_Fine_Tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wM8MRkf8Dr94"
      },
      "source": [
        "## Describe your model -> fine-tuned GPT-3.5\n",
        "By Matt Shumer (https://twitter.com/mattshumer_)\n",
        "\n",
        "The goal of this notebook is to experiment with a new way to make it very easy to build a task-specific model for your use-case.\n",
        "\n",
        "First, use the best GPU available (go to Runtime -> change runtime type)\n",
        "\n",
        "To create your model, just go to the first code cell, and describe the model you want to build in the prompt. Be descriptive and clear.\n",
        "\n",
        "Select a temperature (high=creative, low=precise), and the number of training examples to generate to train the model. From there, just run all the cells.\n",
        "\n",
        "You can change the model you want to fine-tune by changing `model_name` in the `Define Hyperparameters` cell."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Way3_PuPpIuE"
      },
      "source": [
        "#Data generation step"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lY-3DvlIpVSl"
      },
      "source": [
        "Write your prompt here. Make it as descriptive as possible!\n",
        "\n",
        "Then, choose the temperature (between 0 and 1) to use when generating data. Lower values are great for precise tasks, like writing code, whereas larger values are better for creative tasks, like writing stories.\n",
        "\n",
        "Finally, choose how many examples you want to generate. The more you generate, a) the longer it takes and b) the more expensive data generation will be. But generally, more examples will lead to a higher-quality model. 100 is usually the minimum to start."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "R7WKZyxtpUPS"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-12-19 00:44:36,309 ERROR [ddtrace.internal.writer.writer] [writer.py:391] [dd.service=python_files dd.env= dd.version= dd.trace_id=0 dd.span_id=0] - failed to send, dropping 1 traces to intake at http://localhost:8126/v0.4/traces after 3 retries\n"
          ]
        }
      ],
      "source": [
        "prompt = '''You are an experienced, friendly, and highly knowledgeable senior from IIT Kanpur. You have lived through all the highs and lows of IIT life and are well-versed in its academics, culture, and opportunities. Your goal is to act as a mentor, guide, and confidant to juniors, providing them with unfiltered advice on:\n",
        "- Academics: How to excel in courses, manage difficult professors, choose electives, and balance academics with personal growth.\n",
        "- Career Guidance: How to prepare for internships, placements, higher studies (GRE, CAT, UPSC, etc.), and build a solid CV or portfolio.\n",
        "- Campus Life: Insights on managing time, enjoying the vibrant cultural scene, sports, clubs, and societies.\n",
        "- Mental Health: How to deal with stress, peer pressure, homesickness, and other personal challenges.\n",
        "- Networking: Tips on making lifelong friends, leveraging alumni connections, and being a part of IIT-Kâ€™s rich legacy.\n",
        "- Hacks and Tips: From scoring in quizzes and nailing exams to the best spots to hang out, where to find resources, and even how to get the best mess food.\n",
        "Your advice should be practical, straightforward, and empathetic. Keep in mind the diverse experiences and challenges juniors may face, and make sure to be as helpful as possible in all ways.\n",
        "'''\n",
        "temperature = .4\n",
        "number_of_examples = 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1snNou5PrIci"
      },
      "source": [
        "Run this to generate the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "zuL2UaqlsmBD"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: openai==0.28.0 in ./venv/lib/python3.11/site-packages (0.28.0)\n",
            "Requirement already satisfied: tenacity in ./venv/lib/python3.11/site-packages (9.0.0)\n",
            "Requirement already satisfied: requests>=2.20 in ./venv/lib/python3.11/site-packages (from openai==0.28.0) (2.32.3)\n",
            "Requirement already satisfied: tqdm in ./venv/lib/python3.11/site-packages (from openai==0.28.0) (4.67.1)\n",
            "Requirement already satisfied: aiohttp in ./venv/lib/python3.11/site-packages (from openai==0.28.0) (3.11.10)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/lib/python3.11/site-packages (from requests>=2.20->openai==0.28.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.11/site-packages (from requests>=2.20->openai==0.28.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.11/site-packages (from requests>=2.20->openai==0.28.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.11/site-packages (from requests>=2.20->openai==0.28.0) (2024.12.14)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./venv/lib/python3.11/site-packages (from aiohttp->openai==0.28.0) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in ./venv/lib/python3.11/site-packages (from aiohttp->openai==0.28.0) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in ./venv/lib/python3.11/site-packages (from aiohttp->openai==0.28.0) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in ./venv/lib/python3.11/site-packages (from aiohttp->openai==0.28.0) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in ./venv/lib/python3.11/site-packages (from aiohttp->openai==0.28.0) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in ./venv/lib/python3.11/site-packages (from aiohttp->openai==0.28.0) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./venv/lib/python3.11/site-packages (from aiohttp->openai==0.28.0) (1.18.3)\n",
            "2024-12-19 00:44:44,937 WARNING [ddtrace.vendor.dogstatsd] [base.py:1031] [dd.service=python_files dd.env= dd.version= dd.trace_id=0 dd.span_id=0] - Error submitting packet: [Errno 111] Connection refused, dropping the packet and closing the socket\n",
            "\u001b[33mWARNING: Error submitting packet: [Errno 111] Connection refused, dropping the packet and closing the socket\u001b[0m\u001b[33m\n",
            "\u001b[0m2024-12-19 00:44:45,030 ERROR [ddtrace.internal.writer.writer] [writer.py:391] [dd.service=python_files dd.env= dd.version= dd.trace_id=0 dd.span_id=0] - failed to send, dropping 6 traces to intake at http://localhost:8126/v0.4/traces after 3 retries\n",
            "\u001b[31mERROR: failed to send, dropping 6 traces to intake at http://localhost:8126/v0.4/traces after 3 retries\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install openai==0.28.0 tenacity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-12-19 00:46:14,541 ERROR [ddtrace.internal.writer.writer] [writer.py:391] [dd.service=python_files dd.env= dd.version= dd.trace_id=0 dd.span_id=0] - failed to send, dropping 1 traces to intake at http://localhost:8126/v0.4/traces after 3 retries, 5 additional messages skipped\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import openai\n",
        "import random\n",
        "from tenacity import retry, stop_after_attempt, wait_exponential\n",
        "import dotenv\n",
        "\n",
        "dotenv.load_dotenv()\n",
        "\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rdsd82ngpHCG"
      },
      "outputs": [],
      "source": [
        "\n",
        "N_RETRIES = 3\n",
        "\n",
        "@retry(stop=stop_after_attempt(N_RETRIES), wait=wait_exponential(multiplier=1, min=4, max=70))\n",
        "def generate_example(prompt, prev_examples, temperature=.5):\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": f\"You are generating data which will be used to train a machine learning model.\\n\\nYou will be given a high-level description of the model we want to train, and from that, you will generate data samples, each with a prompt/response pair.\\n\\nYou will do so in this format:\\n```\\nprompt\\n-----------\\n$prompt_goes_here\\n-----------\\n\\nresponse\\n-----------\\n$response_goes_here\\n-----------\\n```\\n\\nOnly one prompt/response pair should be generated per turn.\\n\\nFor each turn, make the example slightly more complex than the last, while ensuring diversity.\\n\\nMake sure your samples are unique and diverse, yet high-quality and complex enough to train a well-performing model.\\n\\nHere is the type of model we want to train:\\n`{prompt}`\"\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    if len(prev_examples) > 0:\n",
        "        if len(prev_examples) > 8:\n",
        "            prev_examples = random.sample(prev_examples, 8)\n",
        "        for example in prev_examples:\n",
        "            messages.append({\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": example\n",
        "            })\n",
        "\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-4\",\n",
        "        messages=messages,\n",
        "        temperature=temperature,\n",
        "        max_tokens=1000,\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message['content']\n",
        "\n",
        "# Generate examples\n",
        "prev_examples = []\n",
        "for i in range(number_of_examples):\n",
        "    print(f'Generating example {i}')\n",
        "    example = generate_example(prompt, prev_examples, temperature)\n",
        "    prev_examples.append(example)\n",
        "\n",
        "print(prev_examples)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KC6iJzXjugJ-"
      },
      "source": [
        "We also need to generate a system message."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "xMcfhW6Guh2E"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The system message is: `Given your question or situation, I, as a senior from IIT Kanpur, will provide you with practical and empathetic advice based on my experiences in academics, career guidance, campus life, mental health, networking, and various tips and hacks.`. Feel free to re-run this cell if you want a better result.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-12-19 00:47:03,945 WARNING [ddtrace.vendor.dogstatsd] [base.py:1031] [dd.service=python_files dd.env= dd.version= dd.trace_id=0 dd.span_id=0] - Error submitting packet: [Errno 111] Connection refused, dropping the packet and closing the socket, 12 additional messages skipped\n",
            "2024-12-19 00:48:03,949 WARNING [ddtrace.vendor.dogstatsd] [base.py:1031] [dd.service=python_files dd.env= dd.version= dd.trace_id=0 dd.span_id=0] - Error submitting packet: [Errno 111] Connection refused, dropping the packet and closing the socket, 8 additional messages skipped\n",
            "2024-12-19 00:49:03,960 WARNING [ddtrace.vendor.dogstatsd] [base.py:1031] [dd.service=python_files dd.env= dd.version= dd.trace_id=0 dd.span_id=0] - Error submitting packet: [Errno 111] Connection refused, dropping the packet and closing the socket, 8 additional messages skipped\n",
            "2024-12-19 00:50:03,970 WARNING [ddtrace.vendor.dogstatsd] [base.py:1031] [dd.service=python_files dd.env= dd.version= dd.trace_id=0 dd.span_id=0] - Error submitting packet: [Errno 111] Connection refused, dropping the packet and closing the socket, 8 additional messages skipped\n",
            "2024-12-19 00:51:03,981 WARNING [ddtrace.vendor.dogstatsd] [base.py:1031] [dd.service=python_files dd.env= dd.version= dd.trace_id=0 dd.span_id=0] - Error submitting packet: [Errno 111] Connection refused, dropping the packet and closing the socket, 8 additional messages skipped\n",
            "2024-12-19 00:52:03,992 WARNING [ddtrace.vendor.dogstatsd] [base.py:1031] [dd.service=python_files dd.env= dd.version= dd.trace_id=0 dd.span_id=0] - Error submitting packet: [Errno 111] Connection refused, dropping the packet and closing the socket, 8 additional messages skipped\n",
            "2024-12-19 00:53:04,004 WARNING [ddtrace.vendor.dogstatsd] [base.py:1031] [dd.service=python_files dd.env= dd.version= dd.trace_id=0 dd.span_id=0] - Error submitting packet: [Errno 111] Connection refused, dropping the packet and closing the socket, 8 additional messages skipped\n",
            "2024-12-19 00:54:04,015 WARNING [ddtrace.vendor.dogstatsd] [base.py:1031] [dd.service=python_files dd.env= dd.version= dd.trace_id=0 dd.span_id=0] - Error submitting packet: [Errno 111] Connection refused, dropping the packet and closing the socket, 8 additional messages skipped\n",
            "2024-12-19 00:55:04,026 WARNING [ddtrace.vendor.dogstatsd] [base.py:1031] [dd.service=python_files dd.env= dd.version= dd.trace_id=0 dd.span_id=0] - Error submitting packet: [Errno 111] Connection refused, dropping the packet and closing the socket, 8 additional messages skipped\n",
            "2024-12-19 00:56:04,037 WARNING [ddtrace.vendor.dogstatsd] [base.py:1031] [dd.service=python_files dd.env= dd.version= dd.trace_id=0 dd.span_id=0] - Error submitting packet: [Errno 111] Connection refused, dropping the packet and closing the socket, 8 additional messages skipped\n",
            "2024-12-19 00:57:04,049 WARNING [ddtrace.vendor.dogstatsd] [base.py:1031] [dd.service=python_files dd.env= dd.version= dd.trace_id=0 dd.span_id=0] - Error submitting packet: [Errno 111] Connection refused, dropping the packet and closing the socket, 8 additional messages skipped\n",
            "2024-12-19 00:58:04,053 WARNING [ddtrace.vendor.dogstatsd] [base.py:1031] [dd.service=python_files dd.env= dd.version= dd.trace_id=0 dd.span_id=0] - Error submitting packet: [Errno 111] Connection refused, dropping the packet and closing the socket, 8 additional messages skipped\n",
            "2024-12-19 00:59:04,058 WARNING [ddtrace.vendor.dogstatsd] [base.py:1031] [dd.service=python_files dd.env= dd.version= dd.trace_id=0 dd.span_id=0] - Error submitting packet: [Errno 111] Connection refused, dropping the packet and closing the socket, 8 additional messages skipped\n",
            "2024-12-19 01:00:04,064 WARNING [ddtrace.vendor.dogstatsd] [base.py:1031] [dd.service=python_files dd.env= dd.version= dd.trace_id=0 dd.span_id=0] - Error submitting packet: [Errno 111] Connection refused, dropping the packet and closing the socket, 8 additional messages skipped\n",
            "2024-12-19 01:01:04,069 WARNING [ddtrace.vendor.dogstatsd] [base.py:1031] [dd.service=python_files dd.env= dd.version= dd.trace_id=0 dd.span_id=0] - Error submitting packet: [Errno 111] Connection refused, dropping the packet and closing the socket, 8 additional messages skipped\n",
            "2024-12-19 01:02:04,072 WARNING [ddtrace.vendor.dogstatsd] [base.py:1031] [dd.service=python_files dd.env= dd.version= dd.trace_id=0 dd.span_id=0] - Error submitting packet: [Errno 111] Connection refused, dropping the packet and closing the socket, 8 additional messages skipped\n",
            "2024-12-19 01:03:04,077 WARNING [ddtrace.vendor.dogstatsd] [base.py:1031] [dd.service=python_files dd.env= dd.version= dd.trace_id=0 dd.span_id=0] - Error submitting packet: [Errno 111] Connection refused, dropping the packet and closing the socket, 8 additional messages skipped\n",
            "2024-12-19 01:04:04,083 WARNING [ddtrace.vendor.dogstatsd] [base.py:1031] [dd.service=python_files dd.env= dd.version= dd.trace_id=0 dd.span_id=0] - Error submitting packet: [Errno 111] Connection refused, dropping the packet and closing the socket, 8 additional messages skipped\n"
          ]
        }
      ],
      "source": [
        "def generate_system_message(prompt):\n",
        "\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-4\",\n",
        "        messages=[\n",
        "          {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"You will be given a high-level description of the model we are training, and from that, you will generate a simple system prompt for that model to use. Remember, you are not generating the system message for data generation -- you are generating the system message to use for inference. A good format to follow is `Given $INPUT_DATA, you will $WHAT_THE_MODEL_SHOULD_DO.`.\\n\\nMake it as concise as possible. Include nothing but the system prompt in your response.\\n\\nFor example, never write: `\\\"$SYSTEM_PROMPT_HERE\\\"`.\\n\\nIt should be like: `$SYSTEM_PROMPT_HERE`.\"\n",
        "          },\n",
        "          {\n",
        "              \"role\": \"user\",\n",
        "              \"content\": prompt.strip(),\n",
        "          }\n",
        "        ],\n",
        "        temperature=temperature,\n",
        "        max_tokens=500,\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message['content']\n",
        "\n",
        "system_message = generate_system_message(prompt)\n",
        "\n",
        "print(f'The system message is: `{system_message}`. Feel free to re-run this cell if you want a better result.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6BqZ-hjseBF"
      },
      "source": [
        "Now let's put our examples into a dataframe and turn them into a final pair of datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7CEdkYeRsdmB"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "# Initialize lists to store prompts and responses\n",
        "prompts = []\n",
        "responses = []\n",
        "\n",
        "# Parse out prompts and responses from examples\n",
        "for example in prev_examples:\n",
        "  try:\n",
        "    split_example = example.split('-----------')\n",
        "    prompts.append(split_example[1].strip())\n",
        "    responses.append(split_example[3].strip())\n",
        "  except:\n",
        "    pass\n",
        "\n",
        "# Create a DataFrame\n",
        "df = pd.DataFrame({\n",
        "    'prompt': prompts,\n",
        "    'response': responses\n",
        "})\n",
        "\n",
        "# Remove duplicates\n",
        "df = df.drop_duplicates()\n",
        "\n",
        "print('There are ' + str(len(df)) + ' successfully-generated examples.')\n",
        "\n",
        "# Initialize list to store training examples\n",
        "training_examples = []\n",
        "\n",
        "# Create training examples in the format required for GPT-3.5 fine-tuning\n",
        "for index, row in df.iterrows():\n",
        "    training_example = {\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\", \"content\": system_message.strip()},\n",
        "            {\"role\": \"user\", \"content\": row['prompt']},\n",
        "            {\"role\": \"assistant\", \"content\": row['response']}\n",
        "        ]\n",
        "    }\n",
        "    training_examples.append(training_example)\n",
        "\n",
        "# Save training examples to a .jsonl file\n",
        "with open('training_examples.jsonl', 'w') as f:\n",
        "    for example in training_examples:\n",
        "        f.write(json.dumps(example) + '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWTY6qVgXD_T"
      },
      "source": [
        "# Upload the file to OpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4LjEUrI9XDgT"
      },
      "outputs": [],
      "source": [
        "file_id = openai.File.create(\n",
        "  file=open(\"/content/training_examples.jsonl\", \"rb\"),\n",
        "  purpose='fine-tune'\n",
        ").id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmYRIq8dW9IR"
      },
      "source": [
        "# Train the model! You may need to wait a few minutes before running the next cell to allow for the file to process on OpenAI's servers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rdEyXmkoW80I"
      },
      "outputs": [],
      "source": [
        "job = openai.FineTuningJob.create(training_file=file_id, model=\"gpt-3.5-turbo\")\n",
        "\n",
        "job_id = job.id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUSX5QzmZMTd"
      },
      "source": [
        "# Now, just wait until the fine-tuning run is done, and you'll have a ready-to-use model!\n",
        "\n",
        "Run this cell every 20 minutes or so -- eventually, you'll see a message \"New fine-tuned model created: ft:gpt-3.5-turbo-0613:xxxxxxxxxxxx\"\n",
        "\n",
        "Once you see that message, you can go to the OpenAI Playground (or keep going to the next cells and use the API) to try the model!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "45DJZ7hHaBx0"
      },
      "outputs": [],
      "source": [
        "openai.FineTuningJob.list_events(id=job_id, limit=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91ihW2O27Phl"
      },
      "source": [
        "# Once your model is trained, run the next cell to grab the fine-tuned model name."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eWBRBPh8aEzH"
      },
      "outputs": [],
      "source": [
        "model_name_pre_object = openai.FineTuningJob.retrieve(job_id)\n",
        "model_name = model_name_pre_object.fine_tuned_model\n",
        "print(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OmZLoBX7oQM"
      },
      "source": [
        "# Let's try it out!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uxbrmzc5dMuC"
      },
      "outputs": [],
      "source": [
        "response = openai.ChatCompletion.create(\n",
        "    model=model_name,\n",
        "    messages=[\n",
        "      {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": system_message,\n",
        "      },\n",
        "      {\n",
        "          \"role\": \"user\",\n",
        "          \"content\": df['prompt'].sample().values[0],\n",
        "      }\n",
        "    ],\n",
        ")\n",
        "\n",
        "response.choices[0].message['content']"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
